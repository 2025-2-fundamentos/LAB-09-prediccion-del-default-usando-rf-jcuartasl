{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd0a2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "def load_data(data):\n",
    "    import pandas as pd\n",
    "\n",
    "    data = pd.read_csv(f\"../files/input/{data}.csv.zip\", compression=\"zip\") \n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = load_data(\"train_data\")\n",
    "test_data = load_data(\"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data\n",
    "def clean_data(data):\n",
    "    data = data.copy()\n",
    "    data.rename(columns = {'default payment next month':'default'}, inplace = True)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    data[\"EDUCATION\"] = data[\"EDUCATION\"].apply(lambda x: \"others\" if x not in [1,2,3,4] else str(x))\n",
    "    data.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "cleaned_train_data = clean_data(train_data)\n",
    "cleaned_test_data = clean_data(test_data)\n",
    "\n",
    "x_train = cleaned_train_data.drop(columns=[\"default\"])\n",
    "y_train = cleaned_train_data[\"default\"] \n",
    "x_test = cleaned_test_data.drop(columns=[\"default\"])\n",
    "y_test = cleaned_test_data[\"default\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8915f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "def make_pipeline():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "    cat = [\n",
    "        \"SEX\",\n",
    "        \"EDUCATION\",\n",
    "        \"MARRIAGE\",\n",
    "        \"PAY_0\",\n",
    "        \"PAY_2\",\n",
    "        \"PAY_3\",\n",
    "        \"PAY_4\",\n",
    "        \"PAY_5\",\n",
    "        \"PAY_6\",\n",
    "    ]\n",
    "\n",
    "    \n",
    "    to_str = FunctionTransformer(\n",
    "        func=lambda X: X.astype(str),\n",
    "        validate=False)\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"to_str\", to_str, cat),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=42)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "#Optimización de hiperparametros con validación cruzada\n",
    "def opt_params(pipeline):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'classifier__n_estimators': [200],\n",
    "        'classifier__max_depth': [10, None],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        params, \n",
    "        cv=10, \n",
    "        scoring='balanced_accuracy', \n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "        )\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "\n",
    "pipeline = make_pipeline()\n",
    "estimator = opt_params(pipeline)\n",
    "\n",
    "#Guardar modelo\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists(\"./files/models/\"):\n",
    "    os.makedirs(\"./files/models/\")\n",
    "\n",
    "\n",
    "model_name = \"./files/models/model.pkl.gz\"\n",
    "with gzip.open(model_name, 'wb') as f:\n",
    "    pickle.dump(estimator, f)\n",
    "\n",
    "\n",
    "#Calculo de métricas\n",
    "def calc_metrics(estimator):\n",
    "\n",
    "    x_train = cleaned_train_data.drop(columns=[\"default\"])\n",
    "    y_train = cleaned_train_data[\"default\"]\n",
    "    x_test = cleaned_test_data.drop(columns=[\"default\"])\n",
    "    y_test = cleaned_test_data[\"default\"]\n",
    "\n",
    "    from sklearn.metrics import (\n",
    "        precision_score,\n",
    "        balanced_accuracy_score,\n",
    "        recall_score,\n",
    "        f1_score,\n",
    "        confusion_matrix\n",
    "    )\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    out_dir = \"./files/output/\"\n",
    "    out_file = os.path.join(out_dir, \"metrics.json\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # create the json file if it doesn't exist\n",
    "    if not os.path.exists(out_file):\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    with open(\"files/output/metrics.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "\n",
    "        # ---- TRAIN METRICS ----\n",
    "        y_pred_train = estimator.predict(x_train)\n",
    "        metrics_train = {\n",
    "            \"type\": \"metrics\",\n",
    "            \"dataset\": \"train\",\n",
    "            \"precision\": float(precision_score(y_train, y_pred_train)),\n",
    "            \"balanced_accuracy\": float(balanced_accuracy_score(y_train, y_pred_train)),\n",
    "            \"recall\": float(recall_score(y_train, y_pred_train)),\n",
    "            \"f1_score\": float(f1_score(y_train, y_pred_train)),\n",
    "        }\n",
    "        file.write(json.dumps(metrics_train) + \"\\n\")\n",
    "\n",
    "        cm = confusion_matrix(y_train, y_pred_train)\n",
    "        cm_train = {\n",
    "            \"type\": \"cm_matrix\",\n",
    "            \"dataset\": \"train\",\n",
    "            \"true_0\": {\n",
    "                \"predicted_0\": int(cm[0][0]),\n",
    "                \"predicted_1\": int(cm[0][1]),\n",
    "            },\n",
    "            \"true_1\": {\n",
    "                \"predicted_0\": int(cm[1][0]),\n",
    "                \"predicted_1\": int(cm[1][1]),\n",
    "            },\n",
    "        }\n",
    "        file.write(json.dumps(cm_train) + \"\\n\")\n",
    "\n",
    "        y_pred_test = estimator.predict(x_test)\n",
    "        metrics_test = {\n",
    "            \"type\": \"metrics\",\n",
    "            \"dataset\": \"test\",\n",
    "            \"precision\": float(precision_score(y_test, y_pred_test)),\n",
    "            \"balanced_accuracy\": float(balanced_accuracy_score(y_test, y_pred_test)),\n",
    "            \"recall\": float(recall_score(y_test, y_pred_test)),\n",
    "            \"f1_score\": float(f1_score(y_test, y_pred_test)),\n",
    "        }\n",
    "        file.write(json.dumps(metrics_test) + \"\\n\")\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred_test)\n",
    "        cm_test = {\n",
    "            \"type\": \"cm_matrix\",\n",
    "            \"dataset\": \"test\",\n",
    "            \"true_0\": {\n",
    "                \"predicted_0\": int(cm[0][0]),\n",
    "                \"predicted_1\": int(cm[0][1]),\n",
    "            },\n",
    "            \"true_1\": {\n",
    "                \"predicted_0\": int(cm[1][0]),\n",
    "                \"predicted_1\": int(cm[1][1]),\n",
    "            },\n",
    "        }\n",
    "        file.write(json.dumps(cm_test) + \"\\n\")\n",
    "\n",
    "calc_metrics(estimator)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
